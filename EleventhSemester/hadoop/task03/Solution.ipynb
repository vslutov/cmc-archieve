{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Init data\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Parameters\n",
    "n_classes = 2\n",
    "plot_colors = \"ryb\"\n",
    "plot_step = 0.02\n",
    "\n",
    "# Load data\n",
    "iris = load_iris()\n",
    "pair = [0, 1]\n",
    "\n",
    "# We only take the two corresponding features\n",
    "X = iris.data[:, pair]\n",
    "y = iris.target > 0\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train\n",
    "clf = DecisionTreeClassifier(max_depth=3, min_samples_leaf=1).fit(X_train, y_train)\n",
    "\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "                     np.arange(y_min, y_max, plot_step))\n",
    "plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5)\n",
    "\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "cs = plt.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu)\n",
    "\n",
    "plt.xlabel(iris.feature_names[pair[0]])\n",
    "plt.ylabel(iris.feature_names[pair[1]])\n",
    "\n",
    "# Plot the test points\n",
    "for i, color in zip(range(n_classes), plot_colors):\n",
    "    idx = np.where(y_test == i)\n",
    "    plt.scatter(X_test[idx, 0], X_test[idx, 1], c=color, label=iris.target_names[i],\n",
    "                cmap=plt.cm.RdYlBu, edgecolor='black', s=15)\n",
    "\n",
    "plt.suptitle(\"Decision surface of a decision tree using paired features\")\n",
    "plt.legend(loc='lower right', borderpad=0, handletextpad=0)\n",
    "plt.axis(\"tight\")\n",
    "plt.show()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "print(y_train.shape)\n",
    "df = pd.DataFrame(np.hstack((X_train, y_train.reshape((-1, 1)))), index=np.arange(y_train.shape[0]), columns=['sepal_length', 'sepal_width', 'target'])\n",
    "df.to_csv('train_data.csv', index=False)\n",
    "\n",
    "df = pd.DataFrame(np.hstack((X_test, y_test.reshape((-1, 1)))), index=np.arange(y_test.shape[0]), columns=['sepal_length', 'sepal_width', 'target'])\n",
    "df.to_csv('test_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import json\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql import SparkSession\n",
    "import argparse\n",
    "from pyspark.ml import feature\n",
    "\n",
    "class DecisionTreeClassifier:\n",
    "    def __init__(self, input_columns=None, target_column=None, max_depth=3, min_samples_leaf=1, max_bins=10):\n",
    "        self.input_columns = input_columns\n",
    "        self.target_column = target_column\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.max_bins = max_bins\n",
    "\n",
    "        self.tree = {}\n",
    "        \n",
    "    def fit(self, df):\n",
    "        self.tree['percentiles'] = {}\n",
    "        \n",
    "        df = df.cache()\n",
    "        df_size = df.count()\n",
    "        \n",
    "        for attr in self.input_columns:\n",
    "            percentiles = []\n",
    "            \n",
    "            sorted_rdd = df.select(attr).rdd.sortBy(lambda x: x[0], True)\n",
    "            indexed = sorted_rdd.zipWithIndex().map(lambda x: (x[1], x[0]))\n",
    "            \n",
    "            for i in range(1, self.max_bins):\n",
    "                index = i * df_size // self.max_bins\n",
    "                percentiles.append(indexed.lookup(index)[0][0])\n",
    "            \n",
    "            percentiles = sorted(set(percentiles))\n",
    "            bucketizer = feature.Bucketizer(splits=[float('-inf')] + percentiles + [float('inf')],\n",
    "                                            inputCol=attr, outputCol='bucket')\n",
    "            df = bucketizer.transform(df)\n",
    "            df = (df.withColumn(attr, df['bucket'].cast('int'))\n",
    "                  .drop('bucket')\n",
    "                 )\n",
    "            \n",
    "            self.tree['percentiles'][attr] = percentiles\n",
    "        \n",
    "        self.init_node(self.tree, 0, df)\n",
    "        \n",
    "        del self.tree['percentiles']\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def init_node(self, node, depth, part):\n",
    "        n = part.count()\n",
    "        best_gain = float('-inf')\n",
    "        best_attr = None\n",
    "        best_cutoff = None\n",
    "        best_cutoff_index = None\n",
    "        \n",
    "        if depth < self.max_depth and n > self.min_samples_leaf:\n",
    "\n",
    "            for attr in self.input_columns:\n",
    "                stats = part.groupBy(attr).agg(\n",
    "                    f.count('*').alias('count'),\n",
    "                    f.avg(self.target_column).alias('avg')\n",
    "                )\n",
    "\n",
    "                stats = sorted(tuple(x) for x in stats.collect())\n",
    "                cutoff = [x[0] for x in stats]\n",
    "                count = [x[1] for x in stats]\n",
    "                weight = [x[2] for x in stats]\n",
    "\n",
    "                for i in range(1, len(stats)):\n",
    "                    n_l = sum(count[:i])\n",
    "                    n_r = sum(count[i:])\n",
    "                    if n_l < self.min_samples_leaf or n_r < self.min_samples_leaf:\n",
    "                        continue\n",
    "\n",
    "                    p_l = sum(a * b for a, b in zip(count[:i], weight[:i])) / n_l\n",
    "                    p_r = sum(a * b for a, b in zip(count[i:], weight[i:])) / n_r\n",
    "\n",
    "                    q_l = 1 - p_l\n",
    "                    q_r = 1 - p_r\n",
    "\n",
    "                    gain = -(1 - p_l ** 2 - q_l ** 2) * n_l / n - (1 - p_r ** 2 - q_r ** 2) * n_r / n\n",
    "\n",
    "                    if gain > best_gain:\n",
    "                        best_gain  = gain\n",
    "                        best_attr = attr\n",
    "                        best_cutoff = self.tree['percentiles'][attr][cutoff[i] - 1]\n",
    "                        best_cutoff_index = cutoff[i]\n",
    "            \n",
    "        if best_attr is not None:\n",
    "            node['attr'], node['cutoff'] = best_attr, best_cutoff\n",
    "            node['left'], node['right'] = {}, {}\n",
    "            \n",
    "            left_part = part.filter(part[best_attr] < best_cutoff_index)\n",
    "            self.init_node(node['left'], depth + 1, left_part)\n",
    "            right_part = part.filter(part[best_attr] >= best_cutoff_index)\n",
    "            self.init_node(node['right'], depth + 1, right_part)\n",
    "        else:\n",
    "            node['weight'] = part.groupBy().avg(self.target_column).collect()[0][0]\n",
    "\n",
    "    def predict_proba(self, df):\n",
    "        return df.rdd.map(lambda row: predict_node(self.tree, row))\n",
    "    \n",
    "    def save(self, filename, spark):\n",
    "        spark.sparkContext.parallelize((json.dumps(self.tree), )).saveAsTextFile(filename)\n",
    "        return self\n",
    "            \n",
    "    def load(self, filename, spark):\n",
    "        self.tree = json.loads(spark.sparkContext.textFile(filename).first())\n",
    "        return self\n",
    "    \n",
    "def predict_node(node, row):\n",
    "    if 'attr' in node:\n",
    "        choice = row[node['attr']] >= node['cutoff']\n",
    "        next_node = node['right'] if choice else node['left']\n",
    "        return predict_node(next_node, row)\n",
    "    else:\n",
    "        return (1 - node['weight'], node['weight'])\n",
    "    \n",
    "def parse_list(l):\n",
    "    return l.split(',')\n",
    "\n",
    "def ftrain(spark, args):\n",
    "    train_data = spark.read.csv(args.train_data, header=True)\n",
    "    train_data = train_data.select(\n",
    "        train_data[args.target_column].cast('int'),\n",
    "        *(train_data[col].cast('float') for col in args.input_columns)\n",
    "    )\n",
    "    \n",
    "    clf = DecisionTreeClassifier(input_columns=args.input_columns, \n",
    "                                 target_column=args.target_column, \n",
    "                                 max_depth=args.max_depth,\n",
    "                                 min_samples_leaf=args.min_samples_leaf,\n",
    "                                 max_bins=args.max_bins\n",
    "                                )\n",
    "    clf.fit(train_data)\n",
    "    clf.save(args.tree_filename, spark)\n",
    "    return clf\n",
    "\n",
    "def fpredict(spark, args):\n",
    "    test_data = spark.read.csv(args.test_data, header=True)\n",
    "    test_data = test_data.select(\n",
    "        *(test_data[col].cast('float') for col in test_data.columns)\n",
    "    )\n",
    "    \n",
    "    clf = DecisionTreeClassifier().load(args.tree_filename, spark)\n",
    "    proba = clf.predict_proba(test_data)\n",
    "    proba.map(lambda x: (','.join(str(c) for c in x) )).saveAsTextFile(args.predict_filename)\n",
    "    return proba\n",
    "\n",
    "def main(params=None):\n",
    "    parser = argparse.ArgumentParser('Correlation calculator')\n",
    "    subparsers = parser.add_subparsers(help='train or predict')\n",
    "    \n",
    "    train = subparsers.add_parser('train')\n",
    "    train.add_argument('train_data')\n",
    "    train.add_argument('input_columns', type=parse_list)\n",
    "    train.add_argument('target_column')\n",
    "    train.add_argument('max_depth', type=int)\n",
    "    train.add_argument('min_samples_leaf', type=int)\n",
    "    train.add_argument('max_bins', type=int)\n",
    "    train.add_argument('tree_filename')\n",
    "    train.set_defaults(func=ftrain)\n",
    "    \n",
    "    predict = subparsers.add_parser('predict')\n",
    "    predict.add_argument('test_data')\n",
    "    predict.add_argument('tree_filename')\n",
    "    predict.add_argument('predict_filename')\n",
    "    predict.set_defaults(func=fpredict)\n",
    "    \n",
    "    if params is not None:\n",
    "        args = parser.parse_args(params.split())\n",
    "    else:\n",
    "        args = parser.parse_args()\n",
    "    \n",
    "    spark = (SparkSession.builder.appName(\"Solution\")\n",
    "                .getOrCreate())\n",
    "\n",
    "    return args.func(spark, args)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "# print(main('train train_data.csv sepal_length,sepal_width target 3 1 10 tree.json'))\n",
    "# print(main('predict test_data.csv tree.json predict.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf predict.csv/ tree.json/\n",
    "!python dt.py train train_data.csv sepal_length,sepal_width target 3 1 10 tree.json\n",
    "!python dt.py predict test_data.csv tree.json predict.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pyspark.sql.types import FloatType, StructType, StructField\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Parameters\n",
    "n_classes = 2\n",
    "plot_colors = \"ryb\"\n",
    "plot_step = 0.02\n",
    "\n",
    "# Load data\n",
    "iris = load_iris()\n",
    "pair = [0, 1]\n",
    "\n",
    "# We only take the two corresponding features\n",
    "\n",
    "\n",
    "# Train\n",
    "#clf = main('train train_data.csv sepal_length,sepal_width target 3 1 10 tree.json')\n",
    "#proba = main('predict test_data.csv tree.json predict.csv')\n",
    "\n",
    "spark = (SparkSession.builder.appName(\"Solution\")\n",
    "         .getOrCreate())\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "                     np.arange(y_min, y_max, plot_step))\n",
    "plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5)\n",
    "\n",
    "df = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "\n",
    "df = spark.createDataFrame(df.tolist(), StructType([\n",
    "        StructField('sepal_length', FloatType(), False),\n",
    "        StructField('sepal_width', FloatType(), False)\n",
    "]))\n",
    "\n",
    "clf = DecisionTreeClassifier().load('tree.json', spark)\n",
    "proba = clf.predict_proba(df)\n",
    "proba = proba.map(lambda x: x[1] - x[0]).collect()\n",
    "\n",
    "Z = np.array(proba).astype(np.float32).reshape(xx.shape)\n",
    "cs = plt.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu)\n",
    "\n",
    "plt.xlabel(iris.feature_names[pair[0]])\n",
    "plt.ylabel(iris.feature_names[pair[1]])\n",
    "\n",
    "# Plot the test points\n",
    "for i, color in zip(range(n_classes), plot_colors):\n",
    "    idx = np.where(y_test == i)\n",
    "    plt.scatter(X_test[idx, 0], X_test[idx, 1], c=color, label=iris.target_names[i],\n",
    "                cmap=plt.cm.RdYlBu, edgecolor='black', s=15)\n",
    "\n",
    "plt.suptitle(\"Decision surface of a decision tree using paired features\")\n",
    "plt.legend(loc='lower right', borderpad=0, handletextpad=0)\n",
    "plt.axis(\"tight\")\n",
    "plt.show()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(np.hstack((X_train, y_train.reshape((-1, 1)))), index=np.arange(y_train.shape[0]), columns=['sepal_length', 'sepal_width', 'target'])\n",
    "df.to_csv('train_data.csv', index=False)\n",
    "\n",
    "df = pd.DataFrame(np.hstack((X_test, y_test.reshape((-1, 1)))), index=np.arange(y_test.shape[0]), columns=['sepal_length', 'sepal_width', 'target'])\n",
    "df.to_csv('test_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
